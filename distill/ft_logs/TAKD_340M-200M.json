[
    {
      "args": {
        "save": "../GLM/data/checkpoints/pretrain/block_tiny6/ft_takd/ReCoRD/blank-tiny6-ReCoRD-221229_165121.767106",
        "epochs": 5,
        "iteration": 43385,
        "task": "ReCoRD",
        "wsc_negative": false
      },
      "now": "2022-12-31 07:45:18.073611",
      "max_score_dict": {
        "EM": {
          "epoch": 2,
          "iteration": 26031,
          "score_dict": {
            "EM": 24.43,
            "F1": 25.207857142857126
          }
        },
        "F1": {
          "epoch": 2,
          "iteration": 26031,
          "score_dict": {
            "EM": 24.43,
            "F1": 25.207857142857126
          }
        }
      },
      "epoch": -1,
      "*cmd": "NCCL_DEBUG=info NCCL_IB_DISABLE=0 NCCL_NET_GDR_LEVEL=2 deepspeed --master_port=30948 --include=localhost:6 --hostfile= distill/finetune.py --finetune --cloze-eval --experiment-name=blank-tiny6-ReCoRD-221229_165121.767106 --task=ReCoRD --data-dir=../GLM/data/english_data/superglue/ReCoRD --save=../GLM/data/checkpoints/pretrain/block_tiny6/ft_takd/ReCoRD --seq-length=512 --checkpoint-activations --eval-batch-size=16 --save-epoch=100000 --block-lm --num-layers=18 --hidden-size=896 --num-attention-heads=14 --max-position-embeddings=512 --tokenizer-model-type=bert-base-uncased --tokenizer-type=BertWordPieceTokenizer --load-pretrained= --fp16 --lr-decay-style=linear --warmup=0.1 --weight-decay=1.0e-1 --pattern-id=0 --save-interval=10000 --log-interval=50 --eval-interval=1000 --eval-iters=100 --batch-size=16 --epochs=5 --lr=1e-5 --overwrite --deepspeed-activation-checkpointing --deepspeed --deepspeed_config=config_tasks/config_blocklm_tiny6.json --student_model=kd --student_truncate_tn=0 --distill_ft_soft --distill_ft_soft_kl --distill_ft_hard --distill_temperature=10 --seed=1234 --teacher_load_pretrained=../GLM/data/checkpoints/pretrain/blocklm-large-blank/finetune/ReCoRD/blank-large-ReCoRD-220813_190003 --teacher_num_layers=24 --teacher_hidden_size=1024 --teacher_num_attention_heads=16 --teacher_max_position_embeddings=512 --custom_tmp_result=../GLM/data/tmp/result_221229_165121.767729.json"
    },
    {
      "args": {
        "save": "../GLM/data/checkpoints/pretrain/block_tiny6/ft_takd/COPA/blank-tiny6-COPA-221231_074522.812885",
        "epochs": 50,
        "iteration": 2500,
        "task": "COPA",
        "wsc_negative": false
      },
      "now": "2022-12-31 08:00:01.695387",
      "max_score_dict": {
        "accuracy": {
          "epoch": 2,
          "iteration": 150,
          "score_dict": {
            "accuracy": 61.0
          }
        }
      },
      "epoch": -1,
      "*cmd": "NCCL_DEBUG=info NCCL_IB_DISABLE=0 NCCL_NET_GDR_LEVEL=2 deepspeed --master_port=10661 --include=localhost:6 --hostfile= distill/finetune.py --finetune --cloze-eval --experiment-name=blank-tiny6-COPA-221231_074522.812885 --task=COPA --data-dir=../GLM/data/english_data/superglue/COPA --save=../GLM/data/checkpoints/pretrain/block_tiny6/ft_takd/COPA --seq-length=256 --checkpoint-activations --eval-batch-size=16 --save-epoch=100000 --block-lm --num-layers=18 --hidden-size=896 --num-attention-heads=14 --max-position-embeddings=512 --tokenizer-model-type=bert-base-uncased --tokenizer-type=BertWordPieceTokenizer --load-pretrained= --fp16 --lr-decay-style=linear --warmup=0.1 --weight-decay=1.0e-1 --pattern-id=0 --save-interval=10000 --log-interval=20 --eval-interval=1000 --eval-iters=100 --batch-size=16 --epochs=50 --lr=1e-5 --overwrite --deepspeed-activation-checkpointing --deepspeed --deepspeed_config=config_tasks/config_blocklm_tiny6.json --student_model=kd --student_truncate_tn=0 --distill_ft_soft --distill_ft_soft_kl --distill_ft_hard --distill_temperature=10 --seed=1234 --teacher_load_pretrained=../GLM/data/checkpoints/pretrain/blocklm-large-blank/finetune/COPA/blank-large-COPA-220813_123629 --teacher_num_layers=24 --teacher_hidden_size=1024 --teacher_num_attention_heads=16 --teacher_max_position_embeddings=512 --custom_tmp_result=../GLM/data/tmp/result_221231_074522.819826.json"
    },
    {
      "args": {
        "save": "../GLM/data/checkpoints/pretrain/block_tiny6/ft_takd/WSC/blank-tiny6-WSC-221231_080006.296037",
        "epochs": 50,
        "iteration": 1000,
        "task": "WSC",
        "wsc_negative": true
      },
      "now": "2022-12-31 08:12:39.720294",
      "max_score_dict": {
        "accuracy": {
          "epoch": 14,
          "iteration": 300,
          "score_dict": {
            "accuracy": 65.38461538461539
          }
        }
      },
      "epoch": -1,
      "*cmd": "NCCL_DEBUG=info NCCL_IB_DISABLE=0 NCCL_NET_GDR_LEVEL=2 deepspeed --master_port=22530 --include=localhost:6 --hostfile= distill/finetune.py --finetune --cloze-eval --experiment-name=blank-tiny6-WSC-221231_080006.296037 --task=WSC --data-dir=../GLM/data/english_data/superglue/WSC-negative --save=../GLM/data/checkpoints/pretrain/block_tiny6/ft_takd/WSC --seq-length=128 --checkpoint-activations --eval-batch-size=16 --save-epoch=100000 --block-lm --num-layers=18 --hidden-size=896 --num-attention-heads=14 --max-position-embeddings=512 --tokenizer-model-type=bert-base-uncased --tokenizer-type=BertWordPieceTokenizer --load-pretrained= --fp16 --lr-decay-style=linear --warmup=0.1 --weight-decay=1.0e-1 --loss-func=mix --wsc-negative --length-penalty=1 --pattern-id=2 --save-interval=10000 --log-interval=50 --eval-interval=1000 --eval-iters=100 --batch-size=16 --epochs=50 --lr=1e-5 --overwrite --deepspeed-activation-checkpointing --deepspeed --deepspeed_config=config_tasks/config_blocklm_tiny6.json --student_model=kd --student_truncate_tn=0 --distill_ft_soft --distill_ft_soft_kl --distill_ft_hard --distill_temperature=10 --seed=1234 --teacher_load_pretrained=../GLM/data/checkpoints/pretrain/blocklm-large-blank/finetune/WSC/blank-large-WSC-220813_150605 --teacher_num_layers=24 --teacher_hidden_size=1024 --teacher_num_attention_heads=16 --teacher_max_position_embeddings=512 --custom_tmp_result=../GLM/data/tmp/result_221231_080006.296719.json"
    },
    {
      "args": {
        "save": "../GLM/data/checkpoints/pretrain/block_tiny6/ft_takd/RTE/blank-tiny6-RTE-221231_081244.565957",
        "epochs": 50,
        "iteration": 7800,
        "task": "RTE",
        "wsc_negative": false
      },
      "now": "2022-12-31 08:36:50.126034",
      "max_score_dict": {
        "accuracy": {
          "epoch": 3,
          "iteration": 624,
          "score_dict": {
            "accuracy": 53.068592057761734
          }
        }
      },
      "epoch": -1,
      "*cmd": "NCCL_DEBUG=info NCCL_IB_DISABLE=0 NCCL_NET_GDR_LEVEL=2 deepspeed --master_port=20092 --include=localhost:6 --hostfile= distill/finetune.py --finetune --cloze-eval --experiment-name=blank-tiny6-RTE-221231_081244.565957 --task=RTE --data-dir=../GLM/data/english_data/superglue/RTE --save=../GLM/data/checkpoints/pretrain/block_tiny6/ft_takd/RTE --seq-length=256 --checkpoint-activations --eval-batch-size=16 --save-epoch=100000 --block-lm --num-layers=18 --hidden-size=896 --num-attention-heads=14 --max-position-embeddings=512 --tokenizer-model-type=bert-base-uncased --tokenizer-type=BertWordPieceTokenizer --load-pretrained= --fp16 --lr-decay-style=linear --warmup=0.1 --weight-decay=1.0e-1 --pattern-id=0 --save-interval=10000 --log-interval=50 --eval-interval=10000000 --eval-iters=100 --batch-size=16 --epochs=50 --lr=1e-5 --overwrite --deepspeed-activation-checkpointing --deepspeed --deepspeed_config=config_tasks/config_blocklm_tiny6.json --student_model=kd --student_truncate_tn=0 --distill_ft_soft --distill_ft_soft_kl --distill_ft_hard --distill_temperature=10 --seed=1234 --teacher_load_pretrained=../GLM/data/checkpoints/pretrain/blocklm-large-blank/finetune/RTE/blank-large-RTE-220813_130259 --teacher_num_layers=24 --teacher_hidden_size=1024 --teacher_num_attention_heads=16 --teacher_max_position_embeddings=512 --custom_tmp_result=../GLM/data/tmp/result_221231_081244.566589.json"
    },
    {
      "args": {
        "save": "../GLM/data/checkpoints/pretrain/block_tiny6/ft_takd/BoolQ/blank-tiny6-BoolQ-221231_083652.977484",
        "epochs": 20,
        "iteration": 11800,
        "task": "BoolQ",
        "wsc_negative": false
      },
      "now": "2022-12-31 09:13:48.722180",
      "max_score_dict": {
        "accuracy": {
          "epoch": 9,
          "iteration": 5900,
          "score_dict": {
            "accuracy": 65.13761467889908
          }
        }
      },
      "epoch": -1,
      "*cmd": "NCCL_DEBUG=info NCCL_IB_DISABLE=0 NCCL_NET_GDR_LEVEL=2 deepspeed --master_port=19600 --include=localhost:6 --hostfile= distill/finetune.py --finetune --cloze-eval --experiment-name=blank-tiny6-BoolQ-221231_083652.977484 --task=BoolQ --data-dir=../GLM/data/english_data/superglue/BoolQ --save=../GLM/data/checkpoints/pretrain/block_tiny6/ft_takd/BoolQ --seq-length=256 --checkpoint-activations --eval-batch-size=16 --save-epoch=100000 --block-lm --num-layers=18 --hidden-size=896 --num-attention-heads=14 --max-position-embeddings=512 --tokenizer-model-type=bert-base-uncased --tokenizer-type=BertWordPieceTokenizer --load-pretrained= --fp16 --lr-decay-style=linear --warmup=0.1 --weight-decay=1.0e-1 --pattern-id=4 --save-interval=10000 --log-interval=50 --eval-interval=10000000 --eval-iters=100 --batch-size=16 --epochs=20 --lr=1e-5 --overwrite --deepspeed-activation-checkpointing --deepspeed --deepspeed_config=config_tasks/config_blocklm_tiny6.json --student_model=kd --student_truncate_tn=0 --distill_ft_soft --distill_ft_soft_kl --distill_ft_hard --distill_temperature=10 --seed=1234 --teacher_load_pretrained=../GLM/data/checkpoints/pretrain/blocklm-large-blank/finetune/BoolQ/blank-large-BoolQ-220813_133458 --teacher_num_layers=24 --teacher_hidden_size=1024 --teacher_num_attention_heads=16 --teacher_max_position_embeddings=512 --custom_tmp_result=../GLM/data/tmp/result_221231_083652.978875.json"
    },
    {
      "args": {
        "save": "../GLM/data/checkpoints/pretrain/block_tiny6/ft_takd/WiC/blank-tiny6-WiC-221231_091352.511633",
        "epochs": 30,
        "iteration": 10200,
        "task": "WiC",
        "wsc_negative": false
      },
      "now": "2022-12-31 09:46:57.650442",
      "max_score_dict": {
        "accuracy": {
          "epoch": 5,
          "iteration": 2040,
          "score_dict": {
            "accuracy": 56.739811912225704
          }
        }
      },
      "epoch": -1,
      "*cmd": "NCCL_DEBUG=info NCCL_IB_DISABLE=0 NCCL_NET_GDR_LEVEL=2 deepspeed --master_port=17838 --include=localhost:6 --hostfile= distill/finetune.py --finetune --cloze-eval --experiment-name=blank-tiny6-WiC-221231_091352.511633 --task=WiC --data-dir=../GLM/data/english_data/superglue/WiC --save=../GLM/data/checkpoints/pretrain/block_tiny6/ft_takd/WiC --seq-length=256 --checkpoint-activations --eval-batch-size=16 --save-epoch=100000 --block-lm --num-layers=18 --hidden-size=896 --num-attention-heads=14 --max-position-embeddings=512 --tokenizer-model-type=bert-base-uncased --tokenizer-type=BertWordPieceTokenizer --load-pretrained= --fp16 --lr-decay-style=linear --warmup=0.1 --weight-decay=1.0e-1 --pattern-id=1 --save-interval=10000 --log-interval=50 --eval-interval=10000000 --eval-iters=100 --batch-size=16 --epochs=30 --lr=1e-5 --overwrite --deepspeed-activation-checkpointing --deepspeed --deepspeed_config=config_tasks/config_blocklm_tiny6.json --student_model=kd --student_truncate_tn=0 --distill_ft_soft --distill_ft_soft_kl --distill_ft_hard --distill_temperature=10 --seed=1234 --teacher_load_pretrained=../GLM/data/checkpoints/pretrain/blocklm-large-blank/finetune/WiC/blank-large-WiC-220813_142454 --teacher_num_layers=24 --teacher_hidden_size=1024 --teacher_num_attention_heads=16 --teacher_max_position_embeddings=512 --custom_tmp_result=../GLM/data/tmp/result_221231_091352.512601.json"
    },
    {
      "args": {
        "save": "../GLM/data/checkpoints/pretrain/block_tiny6/ft_takd/CB/blank-tiny6-CB-221231_094702.339667",
        "epochs": 50,
        "iteration": 800,
        "task": "CB",
        "wsc_negative": false
      },
      "now": "2022-12-31 09:50:11.027522",
      "max_score_dict": {
        "accuracy": {
          "epoch": 42,
          "iteration": 688,
          "score_dict": {
            "accuracy": 73.21428571428571,
            "f1-macro": 0.6144349477682812
          }
        },
        "f1-macro": {
          "epoch": 30,
          "iteration": 496,
          "score_dict": {
            "accuracy": 67.85714285714286,
            "f1-macro": 0.6614839061647572
          }
        }
      },
      "epoch": -1,
      "*cmd": "NCCL_DEBUG=info NCCL_IB_DISABLE=0 NCCL_NET_GDR_LEVEL=2 deepspeed --master_port=33291 --include=localhost:6 --hostfile= distill/finetune.py --finetune --cloze-eval --experiment-name=blank-tiny6-CB-221231_094702.339667 --task=CB --data-dir=../GLM/data/english_data/superglue/CB --save=../GLM/data/checkpoints/pretrain/block_tiny6/ft_takd/CB --seq-length=256 --checkpoint-activations --eval-batch-size=16 --save-epoch=100000 --block-lm --num-layers=18 --hidden-size=896 --num-attention-heads=14 --max-position-embeddings=512 --tokenizer-model-type=bert-base-uncased --tokenizer-type=BertWordPieceTokenizer --load-pretrained= --fp16 --lr-decay-style=linear --warmup=0.1 --weight-decay=1.0e-1 --pattern-id=3 --save-interval=10000 --log-interval=50 --eval-interval=10000000 --eval-iters=100 --batch-size=16 --epochs=50 --lr=1e-5 --overwrite --deepspeed-activation-checkpointing --deepspeed --deepspeed_config=config_tasks/config_blocklm_tiny6.json --student_model=kd --student_truncate_tn=0 --distill_ft_soft --distill_ft_soft_kl --distill_ft_hard --distill_temperature=10 --seed=1234 --teacher_load_pretrained=../GLM/data/checkpoints/pretrain/blocklm-large-blank/finetune/CB/blank-large-CB-220813_125843 --teacher_num_layers=24 --teacher_hidden_size=1024 --teacher_num_attention_heads=16 --teacher_max_position_embeddings=512 --custom_tmp_result=../GLM/data/tmp/result_221231_094702.341664.json"
    },
    {
      "args": {
        "save": "../GLM/data/checkpoints/pretrain/block_tiny6/ft_takd/MultiRC/blank-tiny6-MultiRC-221231_095014.058253",
        "epochs": 15,
        "iteration": 25545,
        "task": "MultiRC",
        "wsc_negative": false
      },
      "now": "2022-12-31 12:26:40.274355",
      "max_score_dict": {
        "f1a": {
          "epoch": 12,
          "iteration": 22139,
          "score_dict": {
            "f1a": 0.4609392898052692,
            "em": 0.025183630640083946,
            "acc": 51.46452145214521
          }
        },
        "em": {
          "epoch": 14,
          "iteration": 25545,
          "score_dict": {
            "f1a": 0.39596066967844806,
            "em": 0.03462749213011543,
            "acc": 53.114686468646866
          }
        },
        "acc": {
          "epoch": 4,
          "iteration": 8515,
          "score_dict": {
            "f1a": 0.18556701030927833,
            "em": 0.014690451206715636,
            "acc": 57.632013201320134
          }
        }
      },
      "epoch": -1,
      "*cmd": "NCCL_DEBUG=info NCCL_IB_DISABLE=0 NCCL_NET_GDR_LEVEL=2 deepspeed --master_port=23733 --include=localhost:6 --hostfile= distill/finetune.py --finetune --cloze-eval --experiment-name=blank-tiny6-MultiRC-221231_095014.058253 --task=MultiRC --data-dir=../GLM/data/english_data/superglue/MultiRC --save=../GLM/data/checkpoints/pretrain/block_tiny6/ft_takd/MultiRC --seq-length=512 --checkpoint-activations --eval-batch-size=16 --save-epoch=100000 --block-lm --num-layers=18 --hidden-size=896 --num-attention-heads=14 --max-position-embeddings=512 --tokenizer-model-type=bert-base-uncased --tokenizer-type=BertWordPieceTokenizer --load-pretrained= --fp16 --lr-decay-style=linear --warmup=0.1 --weight-decay=1.0e-1 --pattern-id=0 --save-interval=10000 --log-interval=50 --eval-interval=10000000 --eval-iters=100 --batch-size=16 --epochs=15 --lr=1e-5 --overwrite --deepspeed-activation-checkpointing --deepspeed --deepspeed_config=config_tasks/config_blocklm_tiny6.json --student_model=kd --student_truncate_tn=0 --distill_ft_soft --distill_ft_soft_kl --distill_ft_hard --distill_temperature=10 --seed=1234 --teacher_load_pretrained=../GLM/data/checkpoints/pretrain/blocklm-large-blank/finetune/MultiRC/blank-large-MultiRC-220813_152437 --teacher_num_layers=24 --teacher_hidden_size=1024 --teacher_num_attention_heads=16 --teacher_max_position_embeddings=512 --custom_tmp_result=../GLM/data/tmp/result_221231_095014.058926.json"
    },
    {
      "args": {
        "save": "../GLM/data/checkpoints/pretrain/block_tiny6/ft_takd/WSC/blank-tiny6-WSC_generative-221231_122643.272457",
        "epochs": 50,
        "iteration": 850,
        "task": "WSC",
        "wsc_negative": false
      },
      "now": "2022-12-31 12:29:22.517747",
      "max_score_dict": {
        "accuracy": {
          "epoch": 0,
          "iteration": 17,
          "score_dict": {
            "accuracy": 63.46153846153846
          }
        }
      },
      "epoch": -1,
      "*cmd": "NCCL_DEBUG=info NCCL_IB_DISABLE=0 NCCL_NET_GDR_LEVEL=2 deepspeed --master_port=50587 --include=localhost:6 --hostfile= distill/finetune.py --finetune --cloze-eval --experiment-name=blank-tiny6-WSC_generative-221231_122643.272457 --task=WSC --data-dir=../GLM/data/english_data/superglue/WSC --save=../GLM/data/checkpoints/pretrain/block_tiny6/ft_takd/WSC --seq-length=128 --checkpoint-activations --eval-batch-size=16 --save-epoch=100000 --block-lm --num-layers=18 --hidden-size=896 --num-attention-heads=14 --max-position-embeddings=512 --tokenizer-model-type=bert-base-uncased --tokenizer-type=BertWordPieceTokenizer --load-pretrained= --fp16 --lr-decay-style=linear --warmup=0.1 --weight-decay=1.0e-1 --save-interval=10000 --log-interval=50 --eval-interval=1000 --eval-iters=100 --batch-size=16 --epochs=50 --lr=1e-5 --overwrite --deepspeed-activation-checkpointing --deepspeed --deepspeed_config=config_tasks/config_blocklm_tiny6.json --student_model=kd --student_truncate_tn=0 --distill_ft_soft --distill_ft_soft_kl --distill_ft_hard --distill_temperature=10 --seed=1234 --teacher_load_pretrained=../GLM/data/checkpoints/pretrain/blocklm-large-blank/finetune/WSC/blank-large-WSC_generative-220813_125540 --teacher_num_layers=24 --teacher_hidden_size=1024 --teacher_num_attention_heads=16 --teacher_max_position_embeddings=512 --custom_tmp_result=../GLM/data/tmp/result_221231_122643.273244.json"
    }
  ]