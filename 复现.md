# glm 复现情况
## GLM-base-官方模型(fp16)
### copa
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-copa-07-29-09-10 --task COPA --data-dir data/english_data/superglue/COPA --save data/checkpoints/pretrain/blocklm-base-blank/finetune/COPA --seq-length 256 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/blocklm-base-blank --fp16 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 0 --save-interval 10000 --log-interval 20 --eval-interval 1000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 652| overall: total = 100 accuracy = 58.0000{'epoch': 194, 'accuracy': 73.0}
### rte
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-rte-07-29-08-43 --task RTE --data-dir data/english_data/superglue/RTE --save data/checkpoints/pretrain/blocklm-base-blank/finetune/RTE --seq-length 256 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/blocklm-base-blank --fp16 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 0 --save-interval 10000 --log-interval 50 --eval-interval 10000000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 458| overall: total = 277 accuracy = 67.5090{'epoch': 264, 'accuracy': 72.20216606498195}
### boolq
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-BoolQ-07-29-10-37 --task BoolQ --data-dir data/english_data/superglue/BoolQ --save data/checkpoints/pretrain/blocklm-base-blank/finetune/BoolQ --seq-length 256 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/blocklm-base-blank --fp16 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 4 --save-interval 10000 --log-interval 50 --eval-interval 10000000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 90| overall: total = 3270 accuracy = 77.2477{'epoch': 90, 'accuracy': 77.24770642201835}
### wic
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-WiC-07-29-10-38 --task WiC --data-dir data/english_data/superglue/WiC --save data/checkpoints/pretrain/blocklm-base-blank/finetune/WiC --seq-length 256 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/blocklm-base-blank --fp16 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 1 --save-interval 10000 --log-interval 50 --eval-interval 10000000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 227| overall: total = 638 accuracy = 64.5768{'epoch': 210, 'accuracy': 66.61442006269593}
### cb
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-CB-07-29-10-40 --task CB --data-dir data/english_data/superglue/CB --save data/checkpoints/pretrain/blocklm-base-blank/finetune/CB --seq-length 256 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/blocklm-base-blank --fp16 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 3 --save-interval 10000 --log-interval 50 --eval-interval 10000000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 479| overall: total = 56 accuracy = 91.0714 f1-macro = 0.8651{'epoch': 479, 'accuracy': 91.07142857142857}
### multirc
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-MultiRC-07-29-10-40 --task MultiRC --data-dir data/english_data/superglue/MultiRC --save data/checkpoints/pretrain/blocklm-base-blank/finetune/MultiRC --seq-length 512 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/blocklm-base-blank --fp16 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 0 --save-interval 10000 --log-interval 50 --eval-interval 10000000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 436| overall: total = 4848 f1a = 0.7193 em = 0.2623 acc = 73.4530 max{'epoch': 338, 'score_dict': {'f1a': 0.7225342619636036, 'em': 0.25603357817418676, 'acc': 74.52557755775578, 'type': 'validation
', 'epoch': 338}}
### wsc_generative
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-WSC_generative-07-29-10-40 --task WSC --data-dir data/english_data/superglue/WSC --save data/checkpoints/pretrain/blocklm-base-blank/finetune/WSC --seq-length 128 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/blocklm-base-blank --fp16 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --save-interval 10000 --log-interval 50 --eval-interval 1000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 801| overall: total = 104 accuracy = 67.3077 max{'epoch': 647, 'score_dict': {'accuracy': 73.07692307692308, 'type': 'validation', 'epoch': 647}}
### wsc
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-WSC-07-29-10-40 --task WSC --data-dir data/english_data/superglue/WSC-negative --save data/checkpoints/pretrain/blocklm-base-blank/finetune/WSC --seq-length 128 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/blocklm-base-blank --fp16 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --loss-func mix --wsc-negative --length-penalty 1 --pattern-id 2 --save-interval 10000 --log-interval 50 --eval-interval 1000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 399| overall: total = 104 accuracy = 75.0000 max{'epoch': 318, 'score_dict': {'accuracy': 77.88461538461539, 'type': 'validation', 'epoch': 318}}
### record
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-ReCoRD-07-29-10-40 --task ReCoRD --data-dir data/english_data/superglue/ReCoRD --save data/checkpoints/pretrain/blocklm-base-blank/finetune/ReCoRD --seq-length 512 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/blocklm-base-blank --fp16 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 0 --save-interval 10000 --log-interval 50 --eval-interval 1000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
- 

## 10B
### task: copa, model: model_blocklm_10B, model_pre: , script: finetune_superglue, ds: True
NCCL_DEBUG=info NCCL_IB_DISABLE=0 NCCL_NET_GDR_LEVEL=2 deepspeed --master_port=51299 --include=localhost:6 --hostfile= finetune_glm.py --finetune --cloze-eval --experiment-name=blocklm-10B-COPA-220927_174706.280540 --task=COPA --data-dir=../GLM/data/english_data/superglue/COPA --save=../GLM/data/checkpoints/pretrain/blocklm-xxlarge/finetune/COPA --seq-length=256 --checkpoint-activations --eval-batch-size=16 --save-epoch=100000 --block-lm --task-mask --num-layers=48 --hidden-size=4096 --num-attention-heads=64 --max-position-embeddings=1024 --tokenizer-type=GPT2BPETokenizer --load-pretrained=../GLM/data/checkpoints/pretrain/blocklm-xxlarge --fp16 --lr-decay-style=linear --warmup=0.1 --weight-decay=1.0e-1 --pattern-id=0 --save-interval=10000 --log-interval=20 --eval-interval=1000 --eval-iters=100 --batch-size=16 --epochs=100 --lr=1e-5 --overwrite --deepspeed-activation-checkpointing --deepspeed --deepspeed_config=config_tasks/config_blocklm_10B_1gpu.json

### task: wsc_generative, model: model_blocklm_10B, model_pre: , script: finetune_superglue, ds: True
NCCL_DEBUG=info NCCL_IB_DISABLE=0 NCCL_NET_GDR_LEVEL=2 deepspeed --master_port=50043 --include=localhost:6 --hostfile= finetune_glm.py --finetune --cloze-eval --experiment-name=blocklm-10B-WSC_generative-220927_174706.280588 --task=WSC --data-dir=../GLM/data/english_data/superglue/WSC --save=../GLM/data/checkpoints/pretrain/blocklm-xxlarge/finetune/WSC --seq-length=128 --checkpoint-activations --eval-batch-size=16 --save-epoch=100000 --block-lm --task-mask --num-layers=48 --hidden-size=4096 --num-attention-heads=64 --max-position-embeddings=1024 --tokenizer-type=GPT2BPETokenizer --load-pretrained=../GLM/data/checkpoints/pretrain/blocklm-xxlarge --fp16 --lr-decay-style=linear --warmup=0.1 --weight-decay=1.0e-1 --save-interval=10000 --log-interval=50 --eval-interval=1000 --eval-iters=100 --batch-size=16 --epochs=100 --lr=1e-5 --overwrite --deepspeed-activation-checkpointing --deepspeed --deepspeed_config=config_tasks/config_blocklm_10B_1gpu.json

### task: cb, model: model_blocklm_10B, model_pre: , script: finetune_superglue, ds: True
NCCL_DEBUG=info NCCL_IB_DISABLE=0 NCCL_NET_GDR_LEVEL=2 deepspeed --master_port=27514 --include=localhost:6 --hostfile= finetune_glm.py --finetune --cloze-eval --experiment-name=blocklm-10B-CB-220927_174706.280631 --task=CB --data-dir=../GLM/data/english_data/superglue/CB --save=../GLM/data/checkpoints/pretrain/blocklm-xxlarge/finetune/CB --seq-length=256 --checkpoint-activations --eval-batch-size=16 --save-epoch=100000 --block-lm --task-mask --num-layers=48 --hidden-size=4096 --num-attention-heads=64 --max-position-embeddings=1024 --tokenizer-type=GPT2BPETokenizer --load-pretrained=../GLM/data/checkpoints/pretrain/blocklm-xxlarge --fp16 --lr-decay-style=linear --warmup=0.1 --weight-decay=1.0e-1 --pattern-id=3 --save-interval=10000 --log-interval=50 --eval-interval=10000000 --eval-iters=100 --batch-size=16 --epochs=100 --lr=1e-5 --overwrite --deepspeed-activation-checkpointing --deepspeed --deepspeed_config=config_tasks/config_blocklm_10B_1gpu.json

### task: rte, model: model_blocklm_10B, model_pre: , script: finetune_superglue, ds: True
NCCL_DEBUG=info NCCL_IB_DISABLE=0 NCCL_NET_GDR_LEVEL=2 deepspeed --master_port=25497 --include=localhost:6 --hostfile= finetune_glm.py --finetune --cloze-eval --experiment-name=blocklm-10B-RTE-220927_174706.280676 --task=RTE --data-dir=../GLM/data/english_data/superglue/RTE --save=../GLM/data/checkpoints/pretrain/blocklm-xxlarge/finetune/RTE --seq-length=256 --checkpoint-activations --eval-batch-size=16 --save-epoch=100000 --block-lm --task-mask --num-layers=48 --hidden-size=4096 --num-attention-heads=64 --max-position-embeddings=1024 --tokenizer-type=GPT2BPETokenizer --load-pretrained=../GLM/data/checkpoints/pretrain/blocklm-xxlarge --fp16 --lr-decay-style=linear --warmup=0.1 --weight-decay=1.0e-1 --pattern-id=0 --save-interval=10000 --log-interval=50 --eval-interval=10000000 --eval-iters=100 --batch-size=16 --epochs=50 --lr=1e-5 --overwrite --deepspeed-activation-checkpointing --deepspeed --deepspeed_config=config_tasks/config_blocklm_10B_1gpu.json

### task: boolq, model: model_blocklm_10B, model_pre: , script: finetune_superglue, ds: True
NCCL_DEBUG=info NCCL_IB_DISABLE=0 NCCL_NET_GDR_LEVEL=2 deepspeed --master_port=49723 --include=localhost:6 --hostfile= finetune_glm.py --finetune --cloze-eval --experiment-name=blocklm-10B-BoolQ-220927_174706.280718 --task=BoolQ --data-dir=../GLM/data/english_data/superglue/BoolQ --save=../GLM/data/checkpoints/pretrain/blocklm-xxlarge/finetune/BoolQ --seq-length=256 --checkpoint-activations --eval-batch-size=16 --save-epoch=100000 --block-lm --task-mask --num-layers=48 --hidden-size=4096 --num-attention-heads=64 --max-position-embeddings=1024 --tokenizer-type=GPT2BPETokenizer --load-pretrained=../GLM/data/checkpoints/pretrain/blocklm-xxlarge --fp16 --lr-decay-style=linear --warmup=0.1 --weight-decay=1.0e-1 --pattern-id=4 --save-interval=10000 --log-interval=50 --eval-interval=10000000 --eval-iters=100 --batch-size=16 --epochs=24 --lr=1e-5 --overwrite --deepspeed-activation-checkpointing --deepspeed --deepspeed_config=config_tasks/config_blocklm_10B_1gpu.json

### task: wic, model: model_blocklm_10B, model_pre: , script: finetune_superglue, ds: True
NCCL_DEBUG=info NCCL_IB_DISABLE=0 NCCL_NET_GDR_LEVEL=2 deepspeed --master_port=26670 --include=localhost:6 --hostfile= finetune_glm.py --finetune --cloze-eval --experiment-name=blocklm-10B-WiC-220927_174706.280760 --task=WiC --data-dir=../GLM/data/english_data/superglue/WiC --save=../GLM/data/checkpoints/pretrain/blocklm-xxlarge/finetune/WiC --seq-length=256 --checkpoint-activations --eval-batch-size=16 --save-epoch=100000 --block-lm --task-mask --num-layers=48 --hidden-size=4096 --num-attention-heads=64 --max-position-embeddings=1024 --tokenizer-type=GPT2BPETokenizer --load-pretrained=../GLM/data/checkpoints/pretrain/blocklm-xxlarge --fp16 --lr-decay-style=linear --warmup=0.1 --weight-decay=1.0e-1 --pattern-id=1 --save-interval=10000 --log-interval=50 --eval-interval=10000000 --eval-iters=100 --batch-size=16 --epochs=40 --lr=1e-5 --overwrite --deepspeed-activation-checkpointing --deepspeed --deepspeed_config=config_tasks/config_blocklm_10B_1gpu.json

### task: wsc, model: model_blocklm_10B, model_pre: , script: finetune_superglue, ds: True
NCCL_DEBUG=info NCCL_IB_DISABLE=0 NCCL_NET_GDR_LEVEL=2 deepspeed --master_port=28225 --include=localhost:6 --hostfile= finetune_glm.py --finetune --cloze-eval --experiment-name=blocklm-10B-WSC-220927_174706.280802 --task=WSC --data-dir=../GLM/data/english_data/superglue/WSC-negative --save=../GLM/data/checkpoints/pretrain/blocklm-xxlarge/finetune/WSC --seq-length=128 --checkpoint-activations --eval-batch-size=16 --save-epoch=100000 --block-lm --task-mask --num-layers=48 --hidden-size=4096 --num-attention-heads=64 --max-position-embeddings=1024 --tokenizer-type=GPT2BPETokenizer --load-pretrained=../GLM/data/checkpoints/pretrain/blocklm-xxlarge --fp16 --lr-decay-style=linear --warmup=0.1 --weight-decay=1.0e-1 --loss-func=mix --wsc-negative --length-penalty=1 --pattern-id=2 --save-interval=10000 --log-interval=50 --eval-interval=1000 --eval-iters=100 --batch-size=16 --epochs=100 --lr=1e-5 --overwrite --deepspeed-activation-checkpointing --deepspeed --deepspeed_config=config_tasks/config_blocklm_10B_1gpu.json

### task: multirc, model: model_blocklm_10B, model_pre: , script: finetune_superglue, ds: True
NCCL_DEBUG=info NCCL_IB_DISABLE=0 NCCL_NET_GDR_LEVEL=2 deepspeed --master_port=58098 --include=localhost:6 --hostfile= finetune_glm.py --finetune --cloze-eval --experiment-name=blocklm-10B-MultiRC-220927_174706.280848 --task=MultiRC --data-dir=../GLM/data/english_data/superglue/MultiRC --save=../GLM/data/checkpoints/pretrain/blocklm-xxlarge/finetune/MultiRC --seq-length=512 --checkpoint-activations --eval-batch-size=16 --save-epoch=100000 --block-lm --task-mask --num-layers=48 --hidden-size=4096 --num-attention-heads=64 --max-position-embeddings=1024 --tokenizer-type=GPT2BPETokenizer --load-pretrained=../GLM/data/checkpoints/pretrain/blocklm-xxlarge --fp16 --lr-decay-style=linear --warmup=0.1 --weight-decay=1.0e-1 --pattern-id=0 --save-interval=10000 --log-interval=50 --eval-interval=10000000 --eval-iters=100 --batch-size=16 --epochs=12 --lr=1e-5 --overwrite --deepspeed-activation-checkpointing --deepspeed --deepspeed_config=config_tasks/config_blocklm_10B_1gpu.json

### task: record, model: model_blocklm_10B, model_pre: , script: finetune_superglue, ds: True
NCCL_DEBUG=info NCCL_IB_DISABLE=0 NCCL_NET_GDR_LEVEL=2 deepspeed --master_port=29357 --include=localhost:6 --hostfile= finetune_glm.py --finetune --cloze-eval --experiment-name=blocklm-10B-ReCoRD-220927_174706.280892 --task=ReCoRD --data-dir=../GLM/data/english_data/superglue/ReCoRD --save=../GLM/data/checkpoints/pretrain/blocklm-xxlarge/finetune/ReCoRD --seq-length=512 --checkpoint-activations --eval-batch-size=16 --save-epoch=100000 --block-lm --task-mask --num-layers=48 --hidden-size=4096 --num-attention-heads=64 --max-position-embeddings=1024 --tokenizer-type=GPT2BPETokenizer --load-pretrained=../GLM/data/checkpoints/pretrain/blocklm-xxlarge --fp16 --lr-decay-style=linear --warmup=0.1 --weight-decay=1.0e-1 --pattern-id=0 --save-interval=10000 --log-interval=50 --eval-interval=1000 --eval-iters=100 --batch-size=16 --epochs=3 --lr=1e-5 --overwrite --deepspeed-activation-checkpointing --deepspeed --deepspeed_config=config_tasks/config_blocklm_10B_1gpu.json


# 蒸馏
## 预训练
-deepspeed --master_port=12369 --include=localhost:6 distill_tinybert/pretrain.py --deepspeed_config=config/config_block_tiny6.json --deepspeed-activation-checkpointing --deepspeed --block-lm --num-layers=6 --hidden-size=768 --num-attention-heads=12 --max-position-embeddings=512 --tokenizer-model-type=bert-base-uncased --tokenizer-type=BertWordPieceTokenizer --fp16 --checkpoint-activations --model-parallel-size=1 --save-interval=5000 --save=../GLM/data/checkpoints/distill/tiny6 --experiment-name=test --bert-prob=1.0 --train-data=bert-base --split=949,50,1 --distributed-backend=nccl --lr-decay-style=cosine --lr-decay-iters=120000 --lr-decay-ratio=0.05 --warmup=.05 --train-iters=123456789 --no-lazy-loader --resume-dataloader --has_teacher --teacher_num_layers=12 --teacher_hidden_size=768 --teacher_num_attention_heads=12 --teacher_max_position_embeddings=512 --load-pretrained=../GLM/data/checkpoints/pretrain/blocklm-base-blank
## 微调
- deepspeed --master_port=12361 --include=localhost:6 distill_tinybert/finetune.py --deepspeed_config=config/config_block_tiny6.json --deepspeed-activation-checkpointing --deepspeed --block-lm --num-layers=6 --hidden-size=768 --num-attention-heads=12 --max-position-embeddings=512 --tokenizer-model-type=bert-base-uncased --tokenizer-type=BertWordPieceTokenizer --fp16 --checkpoint-activations --model-parallel-size=1 --save-interval=5000 --task=COPA --data-dir=../GLM/data/english_data/superglue/COPA --load-pretrained=../GLM/data/checkpoints/distill/tiny6/pre-distill6+wikibook19G --save=../GLM/data/checkpoints/distill/tiny6/pre-distill6+wikibook19G/finetune/COPA --experiment-name=tiny6-COPA-220805_141454 --seq-length=256 --batch-size=16 --epochs=700 --lr-decay-style=linear --warmup=0.1 --weight-decay=1.0e-1 --pattern-id=0 --log-interval=50 --lr=1e-5 --overwrite --finetune --cloze-eval --eval-batch-size=16 --save-epoch=100000 --teacher_load_pretrained=../GLM/data/checkpoints/pretrain/blocklm-base-blank --teacher_load_pretrained=../GLM/data/checkpoints/pretrain/blocklm-base-blank/finetune/COPA/blank-base-copa-07-29-09-10 --teacher_num_layers=12 --teacher_hidden_size=768 --teacher_num_attention_heads=12 --teacher_max_position_embeddings=512